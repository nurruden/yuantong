"""
Excelå¯¼å…¥å·¥å…·æ¨¡å—
æä¾›é€šç”¨çš„Excelå¯¼å…¥åŠŸèƒ½ï¼Œç”¨äºQCæŠ¥è¡¨ç­‰æ¨¡å—çš„æ•°æ®å¯¼å…¥
"""
import logging
from datetime import datetime, date, time
from django.http import JsonResponse

logger = logging.getLogger(__name__)


# ==================== å…´è¾‰æŠ¥è¡¨åˆ—åæ˜ å°„é…ç½® ====================

XINGHUI_COLUMN_MAPPING = {
    # åŸºæœ¬å­—æ®µæ˜ å°„
    'æ—¥æœŸ': 'date', 'Date': 'date', 'æ£€æµ‹æ—¥æœŸ': 'date',
    'æ—¶é—´': 'time', 'Time': 'time', 'æ£€æµ‹æ—¶é—´': 'time',
    'ç­æ¬¡': 'shift', 'Shift': 'shift', 'Squad': 'shift', 'ç­ç»„': 'shift', 'ç­åˆ«': 'shift',
    'äº§å“åç§°': 'product_name', 'Product Name': 'product_name', 'Grade': 'product_name', 
    'äº§å“å‹å·': 'product_name',
    'åŒ…è£…ç±»å‹': 'packaging', 'Packaging': 'packaging', 'IPKP CODE': 'packaging',
    'æ‰¹å·': 'batch_number', 'Batch Number': 'batch_number', 'æ‰¹æ¬¡å·': 'batch_number', 
    'LOT': 'batch_number', 'LOTæ‰¹å·': 'batch_number', 'æ‰¹å·/æ—¥æœŸ': 'batch_number', 'æ‰¹æ¬¡': 'batch_number',
    
    # æ£€æµ‹æ•°æ®å­—æ®µæ˜ å°„
    'çƒ˜å¹²ååŸåœŸæ°´åˆ† (%)': 'moisture_after_drying',
    'çƒ˜å¹²ååŸåœŸæ°´åˆ†ï¼ˆ%ï¼‰': 'moisture_after_drying',
    'çƒ˜å¹²ååŸåœŸæ°´åˆ†(%)': 'moisture_after_drying',
    'å¹²ç‡¥ååŸåœŸæ°´åˆ†(%)': 'moisture_after_drying',
    'å¹²ç‡¥ååŸåœŸæ°´åˆ†ï¼ˆ%ï¼‰': 'moisture_after_drying',
    'çƒ˜å¹²ååŸ': 'moisture_after_drying',
    'Moisture after drying': 'moisture_after_drying', 
    'å¹²ç‡¥ååŸåœŸæ°´åˆ†': 'moisture_after_drying',
    
    # å…¥çª‘å‰ç¢±å«é‡
    'å…¥çª‘å‰ç¢±å«é‡(%)': 'alkali_content', 
    'å…¥çª‘å‰ç¢±å«é‡': 'alkali_content',
    'å…¥çª‘å‰': 'alkali_content',
    'å«é‡ (%)': 'alkali_content',
    'Alkali content (%)': 'alkali_content',
    
    'åŠ©å‰‚æ·»åŠ æ¯”ä¾‹': 'flux',
    'åŠ©æº¶å‰‚æ·»åŠ æ¯”ä¾‹': 'flux', 
    'åŠ©æº¶å‰‚': 'flux', 
    'åŠ©ç£¨å‰‚æ·»åŠ æ¯”ä¾‹': 'flux',
    '*flux agent': 'flux', 
    'flux agent addition ratio': 'flux',
    
    # æ¸—é€ç‡ - å…´è¾‰æœ‰ä¸‰ä¸ªæ¸—é€ç‡å­—æ®µ
    'è¿é€šæ»¤é€Ÿç‡': 'permeability',
    'è¿ˆé€šæ¸—é€ç‡': 'permeability',
    'è¿œé€šæ¸—é€ç‡(Darcy)': 'permeability', 
    'è¿œé€šæ¸—é€ç‡': 'permeability',
    'é•¿é«˜æ»¤é€Ÿç‡': 'permeability_long',
    'é•¿å¯Œæ¸—é€ç‡(Darcy)': 'permeability_long', 
    'é•¿å¯Œæ¸—é€ç‡': 'permeability_long',
    'å…´æ¡æ¸—é€ç‡': 'xinghui_permeability',
    'å…´è¾‰æ¸—é€ç‡(Darcy)': 'xinghui_permeability',
    'å…´è¾‰æ¸—é€ç‡': 'xinghui_permeability',
    
    # å¯å¡‘åº¦å¯èƒ½æ˜¯æ¶¡å€¼
    'å¯å¡‘åº¦ (c/cm)': 'swirl',
    'å¯å¡‘åº¦': 'swirl',
    'æ¶¡å€¼(cm)': 'swirl',
    'æ¶¡å€¼ï¼ˆcmï¼‰': 'swirl',
    'æ»¤å€¼(cm)': 'swirl',
    'æ¶¡å€¼': 'swirl', 
    'Swirl (cm)': 'swirl', 
    'Swirl': 'swirl',
    
    # é¥¼å¯†åº¦å’ŒæŒ¯å®å¯†åº¦
    'é¥¼å¯†åº¦(g/cm3)': 'wet_cake_density', 
    'é¥¼å¯†åº¦ï¼ˆg/cm3ï¼‰': 'wet_cake_density',
    'é¥¼å¯†åº¦': 'wet_cake_density', 
    'ç­›å¯†åº¦(g/cm3)': 'wet_cake_density',
    'Wet cake density': 'wet_cake_density',
    'æŒ¯å®å¯†åº¦(g/cm3)': 'bulk_density',
    'æŒ¯å®å¯†åº¦ï¼ˆg/cm3)': 'bulk_density',
    'æŒ¯å®å¯†åº¦ï¼ˆg/cm3ï¼‰': 'bulk_density',
    'æŒ¯å®å¯†åº¦': 'bulk_density',
    'é‡åº¦ (k) 14W': 'bulk_density',
    'ç°å€¼ (c/m)': 'bulk_density',
    
    # ç™½åº¦
    'ç™½åº¦': 'brightness', 
    'Bri.': 'brightness', 
    'Brightness': 'brightness',
    
    'æ°”å‘³': 'odor', 
    'Odor': 'odor',
    
    # ç”µå¯¼å€¼å’ŒpH
    'ç”µå¯¼å€¼ (as/c pH': 'conductance',
    'ç”µå¯¼å€¼(ms/cm)': 'conductance', 
    'ç”µå¯¼å€¼(ms/c pll': 'conductance',
    'ç”µå¯¼å€¼': 'conductance', 
    'Conductance (ms/c)': 'conductance', 
    'Conductance': 'conductance',
    'pH': 'ph', 
    'pHå€¼': 'ph',
    
    'æ°´åˆ†(%)': 'moisture', 
    'æ°´åˆ†': 'moisture', 
    'Moisture (%)': 'moisture', 
    'Moisture': 'moisture',
    
    'æ‰¹æ•°': 'bags',
    'è¢‹æ•°': 'bags', 
    'çƒ§æ•°': 'bags',
    'Bags': 'bags',
    
    'å¨': 'tons', 
    'å¨æ•°': 'tons',
    'Tons': 'tons', 
    'äº§é‡': 'tons',
    
    # ç­›åˆ†æ•°æ®å­—æ®µæ˜ å°„
    '14W': 'sieving_14m',
    '+14M (%)': 'sieving_14m', 
    '+14M': 'sieving_14m', 
    '14M': 'sieving_14m',
    '+30M (%)': 'sieving_30m', 
    '+30M': 'sieving_30m', 
    '30M': 'sieving_30m',
    '+40M (%)': 'sieving_40m', 
    '+40M': 'sieving_40m', 
    '40M': 'sieving_40m',
    'M': 'sieving_40m',
    '+80M (%)': 'sieving_80m', 
    '+80M': 'sieving_80m', 
    '80M': 'sieving_80m',
    '+100M (%)': 'sieving_100m', 
    '+100M': 'sieving_100m', 
    '100M': 'sieving_100m',
    '+150M (%)': 'sieving_150m', 
    '+150M': 'sieving_150m',
    '150M': 'sieving_150m',
    '150M ': 'sieving_150m',
    '+200M (%)': 'sieving_200m', 
    '+200M': 'sieving_200m', 
    '200M': 'sieving_200m',
    '200M ': 'sieving_200m',
    '+325M (%)': 'sieving_325m', 
    '+325M': 'sieving_325m', 
    '325M': 'sieving_325m',
    '325M ': 'sieving_325m',
    
    # ç¦»å­æ•°æ®å­—æ®µæ˜ å°„
    'é“ç¦»å­ (mg/é’™ç¦»å­ (mg/é“ç¦»å­ (mg/ç™½åº¦': 'fe_ion',
    'é“ç¦»å­ï¼ˆmg/kgï¼‰': 'fe_ion',
    'é“ç¦»å­(mg/kg)': 'fe_ion',
    'é“ç¦»å­(mg/': 'fe_ion',
    'é“ç¦»å­': 'fe_ion', 
    'Feç¦»å­': 'fe_ion', 
    'Fe': 'fe_ion',
    'é’™ç¦»å­ï¼ˆmg/kgï¼‰': 'ca_ion',
    'é’™ç¦»å­(mg/kg)': 'ca_ion',
    'é’™ç¦»å­(mg/': 'ca_ion',
    'é’™ç¦»å­': 'ca_ion',
    'Caç¦»å­': 'ca_ion', 
    'Ca': 'ca_ion',
    'é“ç¦»å­ï¼ˆmg/kgï¼‰': 'al_ion',
    'é“ç¦»å­(mg/kg)': 'al_ion',
    'é“ç¦»å­(mg/': 'al_ion',
    'é“ç¦»å­': 'al_ion',
    'Alç¦»å­': 'al_ion', 
    'Al': 'al_ion',
    
    'å¸æ²¹ç‡ (%)': 'oil_absorption',
    'å¸æ²¹ç‡ï¼ˆ%ï¼‰': 'oil_absorption',
    'å¸æ²¹é‡': 'oil_absorption', 
    'å¸æ²¹ç‡(%)': 'oil_absorption',
    'å¸æ°´ç‡ (%)': 'water_absorption',
    'å¸æ°´ç‡ï¼ˆ%ï¼‰': 'water_absorption',
    'å¸æ°´é‡': 'water_absorption', 
    'å¸æ°´ç‡(%)': 'water_absorption',
    
    'å¤‡æ³¨': 'remarks', 
    'Remarks': 'remarks', 
    'Notes': 'remarks'
}

# å­—æ®µæ˜¾ç¤ºåç§°æ˜ å°„
XINGHUI_FIELD_DISPLAY_NAMES = {
    'date': 'æ—¥æœŸ',
    'time': 'æ—¶é—´',
    'shift': 'ç­æ¬¡',
    'product_name': 'äº§å“åç§°',
    'packaging': 'åŒ…è£…ç±»å‹',
    'batch_number': 'æ‰¹å·',
    'moisture_after_drying': 'å¹²ç‡¥ååŸåœŸæ°´åˆ†(%)',
    'alkali_content': 'å…¥çª‘å‰ç¢±å«é‡(%)',
    'flux': 'åŠ©æº¶å‰‚æ·»åŠ æ¯”ä¾‹',
    'permeability': 'è¿œé€šæ¸—é€ç‡(Darcy)',
    'permeability_long': 'é•¿å¯Œæ¸—é€ç‡(Darcy)',
    'xinghui_permeability': 'å…´è¾‰æ¸—é€ç‡(Darcy)',
    'wet_cake_density': 'é¥¼å¯†åº¦(g/cm3)',
    'filter_time': 'è¿‡æ»¤æ—¶é—´(ç§’)',
    'water_viscosity': 'æ°´é»åº¦(mPa.s)',
    'cake_thickness': 'é¥¼åš(mm)',
    'bulk_density': 'æŒ¯å®å¯†åº¦(g/cm3)',
    'brightness': 'ç™½åº¦',
    'swirl': 'æ¶¡å€¼(cm)',
    'odor': 'æ°”å‘³',
    'conductance': 'ç”µå¯¼å€¼(ms/cm)',
    'ph': 'pH',
    'moisture': 'æ°´åˆ†(%)',
    'bags': 'è¢‹æ•°',
    'tons': 'å¨',
    'sieving_14m': '+14M (%)',
    'sieving_30m': '+30M (%)',
    'sieving_40m': '+40M (%)',
    'sieving_80m': '+80M (%)',
    'sieving_100m': '+100M (%)',
    'sieving_150m': '+150M (%)',
    'sieving_200m': '+200M (%)',
    'sieving_325m': '+325M (%)',
    'fe_ion': 'Feç¦»å­',
    'ca_ion': 'Caç¦»å­',
    'al_ion': 'Alç¦»å­',
    'oil_absorption': 'å¸æ²¹é‡',
    'water_absorption': 'å¸æ°´é‡',
    'remarks': 'å¤‡æ³¨',
}


# ==================== é€šç”¨Excelè¯»å–å‡½æ•° ====================

def read_excel_file(excel_file):
    """
    è¯»å–Excelæ–‡ä»¶ï¼Œè¿”å›DataFrameæˆ–ç±»ä¼¼å¯¹è±¡
    
    Args:
        excel_file: Djangoä¸Šä¼ çš„æ–‡ä»¶å¯¹è±¡
        
    Returns:
        tuple: (df, use_pandas) - DataFrameå¯¹è±¡å’Œæ˜¯å¦ä½¿ç”¨pandasçš„æ ‡å¿—
    """
    # å°è¯•å¯¼å…¥pandasï¼Œå¦‚æœå¤±è´¥åˆ™ä½¿ç”¨openpyxl
    try:
        import pandas as pd
        use_pandas = True
    except ImportError:
        use_pandas = False
    
    # è¯»å–Excelæ–‡ä»¶
    if use_pandas:
        try:
            df = pd.read_excel(excel_file, sheet_name=0)
            # åˆ é™¤å®Œå…¨ç©ºç™½çš„è¡Œ
            df = df.dropna(how='all')
            
            # ä¿®å¤ï¼šå¤„ç†åˆ—åä¸pandasæ–¹æ³•åå†²çªçš„é—®é¢˜ï¼ˆå¦‚"shift"ï¼‰
            conflict_names = {'shift': '_shift_field_', 'date': '_date_field_', 'time': '_time_field_'}
            rename_conflicts = {}
            for col in df.columns:
                if col in conflict_names:
                    rename_conflicts[col] = conflict_names[col]
            
            if rename_conflicts:
                df = df.rename(columns=rename_conflicts)
                logger.info(f'âš ï¸ é‡å‘½åå†²çªåˆ—å: {rename_conflicts}')
            
            return df, use_pandas
        except Exception as e:
            raise Exception(f'è¯»å–Excelæ–‡ä»¶å¤±è´¥: {str(e)}')
    else:
        # ä½¿ç”¨openpyxlè¯»å–
        from openpyxl import load_workbook
        try:
            wb = load_workbook(excel_file, data_only=True)
            ws = wb.active
            # è¯»å–è¡¨å¤´
            headers = []
            for cell in ws[1]:
                headers.append(cell.value if cell.value else '')
            
            # å¤„ç†ç‰¹æ®Šåˆ—åæƒ…å†µ
            # 1. å¤„ç†é‡å¤çš„åˆ—åï¼ˆå¦‚ä¸‰ä¸ªæ¸—é€ç‡åˆ—ï¼‰
            permeability_indices = []
            for i, header in enumerate(headers):
                if header and ('Permeability' in str(header) or 'æ¸—é€ç‡' in str(header) or 'æ»¤é€Ÿç‡' in str(header)):
                    permeability_indices.append(i)
            
            # å¦‚æœæœ‰ä¸‰ä¸ªæ¸—é€ç‡åˆ—ï¼Œé‡å‘½åå®ƒä»¬
            if len(permeability_indices) >= 3:
                headers[permeability_indices[0]] = 'Permeability_1'
                headers[permeability_indices[1]] = 'Permeability_2'
                headers[permeability_indices[2]] = 'Permeability_3'
            elif len(permeability_indices) == 2:
                headers[permeability_indices[0]] = 'Permeability_1'
                headers[permeability_indices[1]] = 'Permeability_2'
            
            # 2. å¤„ç†"å…¥çª‘å‰"å’Œ"å«é‡ (%)"åˆå¹¶çš„æƒ…å†µ
            for i, header in enumerate(headers):
                if header and 'å…¥çª‘å‰' in str(header):
                    if i + 1 < len(headers) and headers[i + 1] and 'å«é‡' in str(headers[i + 1]):
                        headers[i + 1] = 'å…¥çª‘å‰ç¢±å«é‡(%)_content'
                    break
            
            # 3. å¤„ç†åˆå¹¶åˆ—"é“ç¦»å­ (mg/é’™ç¦»å­ (mg/é“ç¦»å­ (mg/ç™½åº¦"
            for i, header in enumerate(headers):
                if header and 'é“ç¦»å­' in str(header) and 'é’™ç¦»å­' in str(header):
                    headers[i] = 'é“ç¦»å­_é’™ç¦»å­_é“ç¦»å­_ç™½åº¦_åˆå¹¶åˆ—'
                    break
            
            # 4. å¤„ç†åˆå¹¶åˆ—"ç”µå¯¼å€¼ (as/c pH"
            for i, header in enumerate(headers):
                if header and 'ç”µå¯¼å€¼' in str(header) and 'pH' in str(header):
                    headers[i] = 'ç”µå¯¼å€¼_pH_åˆå¹¶åˆ—'
                    break
            
            # è¯»å–æ•°æ®è¡Œï¼Œè·³è¿‡å®Œå…¨ç©ºç™½çš„è¡Œ
            df_data = []
            logger.info(f'ğŸ“‹ ExcelåŸå§‹åˆ—å: {headers}')
            
            for row in ws.iter_rows(min_row=2, values_only=True):
                if any(cell is not None and str(cell).strip() != '' for cell in row):
                    row_dict = {}
                    for i, header in enumerate(headers):
                        if i < len(row):
                            val = row[i]
                            if val is None or (isinstance(val, str) and val.strip() == ''):
                                row_dict[header] = None
                            else:
                                row_dict[header] = val
                        else:
                            row_dict[header] = None
                    df_data.append(row_dict)
            
            # åˆ›å»ºä¸€ä¸ªç®€å•çš„DataFrameæ¨¡æ‹Ÿå¯¹è±¡
            class SimpleDF:
                def __init__(self, data, columns):
                    self.data = data
                    self.columns = columns
                
                def iterrows(self):
                    for idx, row_dict in enumerate(self.data):
                        class Row:
                            def __init__(self, data):
                                self._data = data
                            
                            def get(self, key, default=None):
                                return self._data.get(key, default)
                            
                            def __getitem__(self, key):
                                return self._data.get(key)
                        
                        yield idx, Row(row_dict)
                
                def __len__(self):
                    return len(self.data)
            
            df = SimpleDF(df_data, headers)
            return df, use_pandas
        except Exception as e:
            raise Exception(f'è¯»å–Excelæ–‡ä»¶å¤±è´¥: {str(e)}')


# ==================== åˆ—åæ˜ å°„å‡½æ•° ====================

def normalize_col_name(col_name):
    """è§„èŒƒåŒ–åˆ—å"""
    if col_name is None:
        return ''
    col_str = str(col_name).strip()
    col_str = col_str.replace('ï¼ˆ', '(').replace('ï¼‰', ')')
    col_str = ' '.join(col_str.split())
    return col_str


def map_excel_columns(df, column_mapping, use_pandas):
    """
    æ˜ å°„Excelåˆ—ååˆ°æ¨¡å‹å­—æ®µå
    
    Args:
        df: DataFrameå¯¹è±¡
        column_mapping: åˆ—åæ˜ å°„å­—å…¸
        use_pandas: æ˜¯å¦ä½¿ç”¨pandas
        
    Returns:
        DataFrame: æ˜ å°„åçš„DataFrame
    """
    if use_pandas and hasattr(df, 'rename'):
        # å¤„ç†é‡å¤çš„åˆ—åï¼ˆå¦‚ä¸‰ä¸ªPermeabilityï¼‰
        permeability_cols = [col for col in df.columns if col and ('Permeability' in str(col) or 'æ¸—é€ç‡' in str(col) or 'æ»¤é€Ÿç‡' in str(col))]
        if len(permeability_cols) >= 3:
            df.columns = [f'Permeability_1' if col == permeability_cols[0] else 
                         f'Permeability_2' if col == permeability_cols[1] else 
                         f'Permeability_3' if col == permeability_cols[2] else col 
                         for col in df.columns]
        elif len(permeability_cols) == 2:
            df.columns = [f'Permeability_1' if col == permeability_cols[0] else 
                         f'Permeability_2' if col == permeability_cols[1] else col 
                         for col in df.columns]
        
        # æ·»åŠ Permeabilityæ˜ å°„
        column_mapping['Permeability_1'] = 'permeability'
        column_mapping['Permeability_2'] = 'permeability_long'
        column_mapping['Permeability_3'] = 'xinghui_permeability'
        
        # ä¿®å¤ï¼šå¤„ç†å†²çªåˆ—åçš„æ˜ å°„ï¼ˆå¦‚_shift_field_æ˜ å°„å›shiftï¼‰
        conflict_mapping = {'_shift_field_': 'shift', '_date_field_': 'date', '_time_field_': 'time'}
        for conflict_col, mapped_field in conflict_mapping.items():
            if conflict_col in df.columns:
                column_mapping[conflict_col] = mapped_field
        
        # è§„èŒƒåŒ–æ˜ å°„
        normalized_mapping = {}
        for orig_key, mapped_value in column_mapping.items():
            normalized_key = normalize_col_name(orig_key)
            normalized_mapping[normalized_key] = mapped_value
        
        rename_dict = {}
        unmapped_cols = []
        
        for col in df.columns:
            col_normalized = normalize_col_name(col)
            if col in column_mapping:
                rename_dict[col] = column_mapping[col]
            elif col_normalized in normalized_mapping:
                rename_dict[col] = normalized_mapping[col_normalized]
            else:
                matched = False
                for orig_key, mapped_value in column_mapping.items():
                    orig_normalized = normalize_col_name(orig_key)
                    if orig_normalized and col_normalized:
                        orig_simple = orig_normalized.replace(' ', '').replace('(', '').replace(')', '').replace('ï¼ˆ', '').replace('ï¼‰', '').lower()
                        col_simple = col_normalized.replace(' ', '').replace('(', '').replace(')', '').replace('ï¼ˆ', '').replace('ï¼‰', '').lower()
                        if orig_simple == col_simple or (len(orig_simple) > 3 and orig_simple in col_simple):
                            rename_dict[col] = mapped_value
                            matched = True
                            break
                
                # ç‰¹æ®Šå¤„ç†"LOTæ‰¹å·"
                if not matched:
                    col_lower = col_normalized.lower()
                    if 'lot' in col_lower and 'æ‰¹å·' in col_normalized:
                        rename_dict[col] = 'batch_number'
                        matched = True
                    elif 'æ‰¹å·' in col_normalized and ('lot' in col_lower or col == 'LOTæ‰¹å·'):
                        rename_dict[col] = 'batch_number'
                        matched = True
                
                if not matched:
                    unmapped_cols.append(col)
        
        df_mapped = df.rename(columns=rename_dict)
        
        if unmapped_cols:
            logger.debug(f'âš ï¸ æœªæ˜ å°„çš„åˆ—å: {unmapped_cols}')
        
        logger.info(f'ğŸ“Š è¯»å–åˆ° {len(df_mapped)} è¡Œæ•°æ®ï¼ŒåŸå§‹åˆ—å: {list(df.columns)}, æ˜ å°„ååˆ—å: {list(df_mapped.columns)}')
        return df_mapped
    else:
        # å¯¹äºopenpyxlï¼Œåˆ—åæ˜ å°„åœ¨è¯»å–æ—¶å·²å¤„ç†
        column_mapping['Permeability_1'] = 'permeability'
        column_mapping['Permeability_2'] = 'permeability_long'
        column_mapping['Permeability_3'] = 'xinghui_permeability'
        logger.info(f'ğŸ“Š è¯»å–åˆ° {len(df)} è¡Œæ•°æ®ï¼Œåˆ—å: {df.columns if hasattr(df, "columns") else "æœªçŸ¥"}')
        return df


# ==================== æ•°æ®æå–å‡½æ•° ====================

def get_row_value(row, key, use_pandas, column_mapping):
    """
    ä»è¡Œä¸­è·å–å€¼ï¼Œæ”¯æŒpandaså’Œopenpyxlä¸¤ç§æ ¼å¼
    
    Args:
        row: è¡Œå¯¹è±¡
        key: å­—æ®µå
        use_pandas: æ˜¯å¦ä½¿ç”¨pandas
        column_mapping: åˆ—åæ˜ å°„å­—å…¸
        
    Returns:
        å­—æ®µå€¼æˆ–None
    """
    if use_pandas:
        try:
            import pandas as pd
            # ä¼˜å…ˆä½¿ç”¨indexè®¿é—®
            if hasattr(row, 'index') and key in row.index:
                val = row[key]
                if callable(val):
                    logger.warning(f'å­—æ®µ {key} è¿”å›äº†æ–¹æ³•å¯¹è±¡ï¼Œå°è¯•å…¶ä»–æ–¹å¼è·å–')
                    val = None
                elif val is not None and (not pd.isna(val) if hasattr(pd, 'isna') else True):
                    return val
            
            if hasattr(row, 'get'):
                val = row.get(key)
                if callable(val):
                    val = None
                elif val is not None and (not pd.isna(val) if hasattr(pd, 'isna') else True):
                    return val
            
            if hasattr(row, key):
                val = getattr(row, key)
                if callable(val):
                    val = None
                elif val is not None and (not pd.isna(val) if hasattr(pd, 'isna') else True):
                    return val
            
            # æ¨¡ç³ŠåŒ¹é…é€»è¾‘
            field_keywords = {
                'date': ['æ—¥æœŸ', 'date', 'Date', 'æ£€æµ‹æ—¥æœŸ'],
                'time': ['æ—¶é—´', 'time', 'Time', 'æ£€æµ‹æ—¶é—´'],
                'shift': ['ç­æ¬¡', 'shift', 'Shift', 'Squad', 'ç­ç»„', 'ç­åˆ«', '_shift_field_'],
                'batch_number': ['æ‰¹å·', 'batch', 'Batch', 'LOT', 'lot', 'æ‰¹æ¬¡å·', 'æ‰¹æ¬¡', 'LOTæ‰¹å·'],
                'moisture_after_drying': ['çƒ˜å¹²å', 'å¹²ç‡¥å', 'åŸåœŸæ°´åˆ†', 'moisture', 'after', 'drying'],
                'permeability': ['è¿œé€š', 'è¿ˆé€š', 'è¿é€š', 'æ¸—é€ç‡', 'permeability', 'Darcy', 'Permeability_1'],
                'permeability_long': ['é•¿å¯Œ', 'é•¿é«˜', 'æ¸—é€ç‡', 'permeability', 'Permeability_2'],
                'xinghui_permeability': ['å…´è¾‰', 'å…´æ¡', 'Permeability_3'],
                'wet_cake_density': ['é¥¼å¯†åº¦', 'ç­›å¯†åº¦', 'cake', 'density'],
                'bulk_density': ['æŒ¯å®å¯†åº¦', 'bulk', 'density'],
                'brightness': ['ç™½åº¦', 'brightness', 'Bri'],
                'swirl': ['æ¶¡å€¼', 'æ»¤å€¼', 'å¯å¡‘åº¦', 'swirl'],
                'odor': ['æ°”å‘³', 'odor'],
                'conductance': ['ç”µå¯¼å€¼', 'conductance'],
                'ph': ['pH', 'ph'],
                'moisture': ['æ°´åˆ†', 'moisture'],
                'bags': ['è¢‹æ•°', 'çƒ§æ•°', 'bags', 'æ‰¹æ•°'],
                'tons': ['å¨', 'tons', 'äº§é‡'],
                'fe_ion': ['é“ç¦»å­', 'Fe', 'fe'],
                'ca_ion': ['é’™ç¦»å­', 'Ca', 'ca'],
                'al_ion': ['é“ç¦»å­', 'Al', 'al'],
                'oil_absorption': ['å¸æ²¹', 'oil'],
                'water_absorption': ['å¸æ°´', 'water'],
                'sieving_14m': ['14M', '14', '+14'],
                'sieving_30m': ['30M', '30', '+30'],
                'sieving_40m': ['40M', '40', '+40'],
                'sieving_80m': ['80M', '80', '+80'],
                'sieving_100m': ['100M', '100', '+100'],
                'sieving_150m': ['150M', '150', '+150'],
                'sieving_200m': ['200M', '200', '+200'],
                'sieving_325m': ['325M', '325', '+325'],
            }
            
            keywords = field_keywords.get(key, [])
            
            if hasattr(row, 'index'):
                sorted_keywords = sorted(keywords, key=lambda k: (k.isascii(), -len(k)))
                
                for keyword in sorted_keywords:
                    best_match_col = None
                    best_match_val = None
                    
                    for col_name in row.index:
                        if col_name is None:
                            continue
                        col_str = str(col_name).strip()
                        
                        if keyword in col_str:
                            # ç‰¹æ®Šå¤„ç†ï¼šå¯¹äºxinghui_permeabilityï¼Œå¿…é¡»ç¡®ä¿åˆ—ååŒ…å«"å…´è¾‰"æˆ–"å…´æ¡"
                            if key == 'xinghui_permeability':
                                if 'å…´è¾‰' not in col_str and 'å…´æ¡' not in col_str and 'Permeability_3' not in col_str:
                                    continue
                            
                            try:
                                val = row[col_name]
                                if val is not None and (not pd.isna(val) if hasattr(pd, 'isna') else True):
                                    if best_match_col is None:
                                        best_match_col = col_name
                                        best_match_val = val
                                    elif len(col_str) < len(str(best_match_col)):
                                        best_match_col = col_name
                                        best_match_val = val
                            except (KeyError, IndexError):
                                continue
                    
                    if best_match_val is not None:
                        return best_match_val
                    
                    if best_match_col is not None:
                        return None
            
            return None
        except Exception as e:
            logger.debug(f'è·å–å­—æ®µ {key} å¤±è´¥: {str(e)}')
            return None
    else:
        # openpyxlæ ¼å¼å¤„ç†
        if isinstance(row, dict):
            if key in row:
                val = row[key]
                if val is not None:
                    return val
        
        matched_cols = []
        for orig_col, mapped_col in column_mapping.items():
            if mapped_col == key:
                matched_cols.append(orig_col)
        
        for orig_col in matched_cols:
            if isinstance(row, dict) and orig_col in row:
                val = row[orig_col]
                if val is not None:
                    return val
        
        if isinstance(row, dict):
            key_keywords = {
                'batch_number': ['æ‰¹å·', 'batch', 'Batch', 'LOT', 'lot', 'æ‰¹æ¬¡å·', 'æ‰¹æ¬¡', 'LOTæ‰¹å·'],
                'moisture_after_drying': ['çƒ˜å¹²å', 'å¹²ç‡¥å', 'åŸåœŸæ°´åˆ†', 'moisture', 'after', 'drying'],
                'permeability': ['è¿œé€š', 'è¿ˆé€š', 'è¿é€š', 'æ¸—é€ç‡', 'permeability', 'Darcy'],
                'permeability_long': ['é•¿å¯Œ', 'é•¿é«˜', 'æ¸—é€ç‡', 'permeability'],
                'xinghui_permeability': ['å…´è¾‰', 'å…´æ¡'],
                'swirl': ['æ¶¡å€¼', 'æ»¤å€¼', 'å¯å¡‘åº¦', 'swirl'],
                'sieving_150m': ['150M', '150', '+150'],
                'sieving_200m': ['200M', '200', '+200'],
                'sieving_325m': ['325M', '325', '+325'],
                'fe_ion': ['é“ç¦»å­', 'Fe', 'fe'],
                'ca_ion': ['é’™ç¦»å­', 'Ca', 'ca'],
                'al_ion': ['é“ç¦»å­', 'Al', 'al'],
                'oil_absorption': ['å¸æ²¹', 'oil'],
                'water_absorption': ['å¸æ°´', 'water'],
                'moisture': ['æ°´åˆ†', 'moisture'],
            }
            
            keywords = key_keywords.get(key, [])
            
            for orig_col in row.keys():
                if orig_col is None:
                    continue
                orig_col_str = str(orig_col).strip()
                
                if keywords:
                    for keyword in keywords:
                        if keyword in orig_col_str:
                            if key == 'xinghui_permeability':
                                if 'å…´è¾‰' not in orig_col_str and 'å…´æ¡' not in orig_col_str:
                                    continue
                            
                            val = row[orig_col]
                            if val is not None:
                                return val
                else:
                    key_normalized = str(key).strip().replace(' ', '').replace('(', '').replace(')', '').replace('ï¼ˆ', '').replace('ï¼‰', '').replace('+', '').lower()
                    orig_col_normalized = orig_col_str.replace(' ', '').replace('(', '').replace(')', '').replace('ï¼ˆ', '').replace('ï¼‰', '').replace('+', '').lower()
                    if key_normalized in orig_col_normalized or orig_col_normalized in key_normalized:
                        val = row[orig_col]
                        if val is not None:
                            return val
        
        return None


def is_notna(val, use_pandas):
    """æ£€æŸ¥å€¼æ˜¯å¦ä¸ä¸ºç©º"""
    if use_pandas:
        import pandas as pd
        return pd.notna(val)
    else:
        return val is not None and val != '' and str(val).strip() != ''


# ==================== æ•°æ®éªŒè¯å‡½æ•° ====================

def is_hint_row(date_val, use_pandas):
    """æ£€æŸ¥æ˜¯å¦æ˜¯æç¤ºä¿¡æ¯è¡Œ"""
    hint_keywords = ['è¯´æ˜', 'æç¤º', 'æ³¨æ„', 'è¯·åˆ é™¤', 'ç¤ºä¾‹', 'hint', 'note', 'è¯´æ˜ï¼š']
    if date_val and is_notna(date_val, use_pandas):
        date_str = str(date_val).strip()
        for keyword in hint_keywords:
            if keyword in date_str:
                return True
    return False


def has_valid_data(row, get_row_value_func, use_pandas):
    """æ£€æŸ¥è¡Œæ˜¯å¦æœ‰æœ‰æ•ˆæ•°æ®"""
    date_val = get_row_value_func('date')
    product_name_val = get_row_value_func('product_name')
    
    has_any_data = False
    if date_val and is_notna(date_val, use_pandas):
        has_any_data = True
    if product_name_val and is_notna(product_name_val, use_pandas):
        has_any_data = True
    
    if not has_any_data:
        for key in ['shift', 'packaging', 'bags', 'batch_number', 'moisture_after_drying', 
                   'alkali_content', 'permeability', 'permeability_long', 'xinghui_permeability', 'wet_cake_density']:
            val = get_row_value_func(key)
            if val and is_notna(val, use_pandas):
                has_any_data = True
                break
    
    return has_any_data


# ==================== æ•°æ®å¤„ç†å‡½æ•° ====================

def process_date_value(date_val, use_pandas):
    """å¤„ç†æ—¥æœŸå€¼"""
    if date_val and is_notna(date_val, use_pandas):
        if isinstance(date_val, str):
            try:
                return datetime.strptime(date_val.strip(), '%Y-%m-%d').date()
            except:
                try:
                    return datetime.strptime(date_val.strip(), '%Y/%m/%d').date()
                except:
                    return date.today()
        elif hasattr(date_val, 'date'):
            return date_val.date()
        else:
            return date.today()
    return None


def process_time_value(time_val, use_pandas):
    """å¤„ç†æ—¶é—´å€¼"""
    if time_val and is_notna(time_val, use_pandas):
        try:
            if isinstance(time_val, str):
                time_str = str(time_val).strip()
                if not time_str:
                    return time(0, 0)
                else:
                    try:
                        return datetime.strptime(time_str, '%H:%M').time()
                    except ValueError:
                        try:
                            return datetime.strptime(time_str, '%H:%M:%S').time()
                        except ValueError:
                            try:
                                if ':' not in time_str and ('.' in time_str or time_str.replace('.', '').replace('-', '').isdigit()):
                                    time_float = float(time_str)
                                    total_seconds = int(time_float * 24 * 3600)
                                    hours = total_seconds // 3600
                                    minutes = (total_seconds % 3600) // 60
                                    seconds = total_seconds % 60
                                    return time(hours, minutes, seconds)
                                else:
                                    if time_str.isdigit() and len(time_str) <= 4:
                                        hours = int(time_str[:2]) if len(time_str) >= 2 else int(time_str[0])
                                        minutes = int(time_str[-2:]) if len(time_str) >= 2 else 0
                                        return time(hours, minutes)
                                    else:
                                        return time(0, 0)
                            except (ValueError, TypeError):
                                return time(0, 0)
            elif hasattr(time_val, 'time'):
                return time_val.time()
            elif isinstance(time_val, time):
                return time_val
            elif hasattr(time_val, 'hour') and hasattr(time_val, 'minute'):
                return time(time_val.hour, time_val.minute, getattr(time_val, 'second', 0))
            elif isinstance(time_val, (int, float)):
                total_seconds = int(time_val * 24 * 3600)
                hours = total_seconds // 3600
                minutes = (total_seconds % 3600) // 60
                seconds = total_seconds % 60
                return time(hours, minutes, seconds)
            else:
                return time(0, 0)
        except Exception as e:
            return time(0, 0)
    return time(0, 0)


def process_numeric_value(val, use_pandas):
    """å¤„ç†æ•°å­—å€¼"""
    if val is not None and is_notna(val, use_pandas):
        try:
            import pandas as pd
            if isinstance(val, str):
                val = val.strip()
                if val == '' or val.isspace():
                    return None
                else:
                    float_val = float(val)
                    if pd.isna(float_val) if hasattr(pd, 'isna') else (float_val != float_val or abs(float_val) == float('inf')):
                        return None
                    else:
                        return float_val
            elif isinstance(val, (int, float)):
                if pd.isna(val) if hasattr(pd, 'isna') else (val != val or abs(val) == float('inf')):
                    return None
                else:
                    return float(val)
            else:
                return float(val)
        except (ValueError, TypeError):
            return None
    return None


# ==================== é€šç”¨å¯¼å…¥å‡½æ•° ====================

def import_xinghui_report_data(request, model_class, module_name, log_module_code):
    """
    é€šç”¨çš„å…´è¾‰æŠ¥è¡¨å¯¼å…¥å‡½æ•°
    
    Args:
        request: Djangoè¯·æ±‚å¯¹è±¡
        model_class: æ¨¡å‹ç±»ï¼ˆXinghuiQCReportæˆ–Xinghui2QCReportï¼‰
        module_name: æ¨¡å—åç§°ï¼ˆç”¨äºé”™è¯¯æç¤ºï¼‰
        log_module_code: æ—¥å¿—æ¨¡å—ä»£ç ï¼ˆ'xinghui'æˆ–'xinghui2'ï¼‰
        
    Returns:
        JsonResponse: å¯¼å…¥ç»“æœ
    """
    if request.method != 'POST':
        return JsonResponse({'status': 'error', 'message': 'ä»…æ”¯æŒPOSTè¯·æ±‚'}, status=405)
    
    try:
        # æ£€æŸ¥æ˜¯å¦æœ‰ä¸Šä¼ çš„æ–‡ä»¶
        if 'excel_file' not in request.FILES:
            return JsonResponse({'status': 'error', 'message': 'è¯·é€‰æ‹©è¦å¯¼å…¥çš„Excelæ–‡ä»¶'}, status=400)
        
        excel_file = request.FILES['excel_file']
        
        # æ£€æŸ¥æ–‡ä»¶æ‰©å±•å
        if not excel_file.name.endswith(('.xlsx', '.xls')):
            return JsonResponse({'status': 'error', 'message': 'ä»…æ”¯æŒExcelæ–‡ä»¶æ ¼å¼(.xlsx, .xls)'}, status=400)
        
        # è¯»å–Excelæ–‡ä»¶
        df, use_pandas = read_excel_file(excel_file)
        
        # æ˜ å°„åˆ—å
        column_mapping = XINGHUI_COLUMN_MAPPING.copy()
        df_mapped = map_excel_columns(df, column_mapping, use_pandas)
        
        # å¤„ç†æ•°æ®å¹¶å¯¼å…¥
        imported_count = 0
        error_count = 0
        error_messages = []
        skipped_count = 0
        
        # å¯¼å…¥validate_field_by_modelå‡½æ•°
        from home.utils.validators import validate_field_by_model
        
        # å¤„ç†æ¯ä¸€è¡Œæ•°æ®
        for index, row_obj in df_mapped.iterrows():
            try:
                row = row_obj
                
                # åˆ›å»ºget_row_valueçš„é—­åŒ…å‡½æ•°
                def get_row_value_func(key):
                    return get_row_value(row, key, use_pandas, column_mapping)
                
                # æ£€æŸ¥æ˜¯å¦æ˜¯æç¤ºä¿¡æ¯è¡Œ
                date_val = get_row_value_func('date')
                if is_hint_row(date_val, use_pandas):
                    skipped_count += 1
                    logger.debug(f'è·³è¿‡ç¬¬ {index + 2} è¡Œï¼šæç¤ºä¿¡æ¯è¡Œ')
                    continue
                
                # æ£€æŸ¥å¿…å¡«å­—æ®µï¼šdateå’Œproduct_name
                product_name_val = get_row_value_func('product_name')
                if not (date_val and is_notna(date_val, use_pandas)):
                    error_count += 1
                    error_msg = f'ç¬¬ {index + 2} è¡Œç¼ºå°‘å¿…å¡«å­—æ®µ: æ—¥æœŸ'
                    error_messages.append(error_msg)
                    logger.error(f'å¯¼å…¥{module_name}å¤±è´¥: {error_msg}')
                    continue
                
                if not (product_name_val and is_notna(product_name_val, use_pandas)):
                    error_count += 1
                    error_msg = f'ç¬¬ {index + 2} è¡Œç¼ºå°‘å¿…å¡«å­—æ®µ: äº§å“åç§°'
                    error_messages.append(error_msg)
                    logger.error(f'å¯¼å…¥{module_name}å¤±è´¥: {error_msg}')
                    continue
                
                # å¤„ç†æ—¥æœŸå’Œæ—¶é—´
                date_obj = process_date_value(date_val, use_pandas)
                if not date_obj:
                    error_count += 1
                    error_msg = f'ç¬¬ {index + 2} è¡Œæ—¥æœŸæ ¼å¼é”™è¯¯'
                    error_messages.append(error_msg)
                    logger.error(f'å¯¼å…¥{module_name}å¤±è´¥: {error_msg}')
                    continue
                
                time_val = get_row_value_func('time')
                time_obj = process_time_value(time_val, use_pandas)
                
                # æ„å»ºæ•°æ®å­—å…¸
                data = {
                    'date': date_obj,
                    'time': time_obj,
                    'product_name': str(product_name_val).strip(),
                }
                
                # å¤„ç†å­—ç¬¦ä¸²å­—æ®µ
                string_fields = ['shift', 'packaging', 'batch_number', 'flux', 'remarks']
                for field in string_fields:
                    val = get_row_value_func(field)
                    if val and is_notna(val, use_pandas):
                        data[field] = str(val)
                    else:
                        data[field] = ''
                
                # å¤„ç†å…¥çª‘å‰ç¢±å«é‡
                alkali_val = get_row_value_func('alkali_content')
                data['alkali_content'] = process_numeric_value(alkali_val, use_pandas)
                
                # å¤„ç†æ•°å­—å­—æ®µ
                numeric_fields = [
                    'moisture_after_drying', 'permeability', 'permeability_long', 'xinghui_permeability',
                    'wet_cake_density', 'bulk_density', 'brightness', 'swirl', 'odor',
                    'conductance', 'ph', 'moisture', 'bags', 'tons', 'fe_ion', 'ca_ion',
                    'al_ion', 'oil_absorption', 'water_absorption', 'sieving_14m', 'sieving_30m',
                    'sieving_40m', 'sieving_80m'
                ]
                
                for field in numeric_fields:
                    val = get_row_value_func(field)
                    data[field] = process_numeric_value(val, use_pandas)
                
                # å¤„ç†åˆå¹¶åˆ—ï¼ˆä»…openpyxlï¼‰
                if not use_pandas:
                    # å¤„ç†é“ç¦»å­/é’™ç¦»å­/é“ç¦»å­/ç™½åº¦åˆå¹¶åˆ—
                    merged_ion_col = None
                    if hasattr(row, 'get'):
                        merged_ion_col = row.get('é“ç¦»å­_é’™ç¦»å­_é“ç¦»å­_ç™½åº¦_åˆå¹¶åˆ—')
                    elif isinstance(row, dict):
                        merged_ion_col = row.get('é“ç¦»å­_é’™ç¦»å­_é“ç¦»å­_ç™½åº¦_åˆå¹¶åˆ—')
                    
                    if merged_ion_col and is_notna(merged_ion_col, use_pandas):
                        merged_str = str(merged_ion_col)
                        if '/' in merged_str:
                            parts = merged_str.split('/')
                            if len(parts) >= 3:
                                try:
                                    if parts[0].strip():
                                        data['fe_ion'] = float(parts[0].strip())
                                    if parts[1].strip():
                                        data['ca_ion'] = float(parts[1].strip())
                                    if parts[2].strip():
                                        data['al_ion'] = float(parts[2].strip())
                                    if len(parts) >= 4 and parts[3].strip():
                                        data['brightness'] = float(parts[3].strip())
                                except:
                                    pass
                    
                    # å¤„ç†ç”µå¯¼å€¼/pHåˆå¹¶åˆ—
                    merged_conductance_col = None
                    if hasattr(row, 'get'):
                        merged_conductance_col = row.get('ç”µå¯¼å€¼_pH_åˆå¹¶åˆ—')
                    elif isinstance(row, dict):
                        merged_conductance_col = row.get('ç”µå¯¼å€¼_pH_åˆå¹¶åˆ—')
                    
                    if merged_conductance_col and is_notna(merged_conductance_col, use_pandas):
                        merged_str = str(merged_conductance_col)
                        if '/' in merged_str or 'pH' in merged_str:
                            parts = merged_str.replace('pH', '').split('/')
                            if len(parts) >= 1 and parts[0].strip():
                                try:
                                    data['conductance'] = float(parts[0].strip())
                                except:
                                    pass
                            import re
                            ph_match = re.search(r'pH[:\s]*([0-9.]+)', merged_str, re.IGNORECASE)
                            if ph_match:
                                try:
                                    data['ph'] = float(ph_match.group(1))
                                except:
                                    pass
                
                # å¤„ç†ç­›åˆ†å­—æ®µï¼ˆå¯èƒ½æ˜¯å­—ç¬¦ä¸²ï¼‰
                sieving_fields = ['sieving_100m', 'sieving_150m', 'sieving_200m', 'sieving_325m']
                for field in sieving_fields:
                    val = get_row_value_func(field)
                    if val and is_notna(val, use_pandas):
                        data[field] = str(val)
                    else:
                        data[field] = ''
                
                # æ•°æ®æ ¡éªŒ
                validation_errors = []
                field_display_names = XINGHUI_FIELD_DISPLAY_NAMES.copy()
                
                for field_name, field_value in data.items():
                    if field_name in ['user', 'username']:
                        continue
                    
                    field_display_name = field_display_names.get(field_name, field_name)
                    is_valid, error_msg = validate_field_by_model(
                        model_class, 
                        field_name, 
                        field_value, 
                        field_display_name
                    )
                    
                    if not is_valid:
                        validation_errors.append(f'{field_display_name}: {error_msg}')
                
                if validation_errors:
                    error_count += 1
                    error_msg = f'ç¬¬ {index + 2} è¡Œæ•°æ®æ ¡éªŒå¤±è´¥: {"; ".join(validation_errors)}'
                    error_messages.append(error_msg)
                    logger.error(f'å¯¼å…¥{module_name}æ•°æ®æ ¡éªŒå¤±è´¥: {error_msg}')
                    continue
                
                # è®¾ç½®ç”¨æˆ·ä¿¡æ¯
                data['user'] = request.user
                data['username'] = request.user.username
                
                # åˆ›å»ºè®°å½•
                model_class.objects.create(**data)
                imported_count += 1
                
            except Exception as e:
                error_count += 1
                error_msg = f'ç¬¬ {index + 2} è¡Œå¯¼å…¥å¤±è´¥: {str(e)}'
                error_messages.append(error_msg)
                logger.error(f'å¯¼å…¥{module_name}å¤±è´¥: {error_msg}', exc_info=True)
        
        # è®°å½•æ“ä½œæ—¥å¿—
        from home.models import UserOperationLog
        UserOperationLog.log_operation(
            request, 'CREATE', log_module_code, None,
            f'æ‰¹é‡å¯¼å…¥Excelæ•°æ®: æˆåŠŸ{imported_count}æ¡, å¤±è´¥{error_count}æ¡'
        )
        
        result = {
            'status': 'success',
            'message': f'å¯¼å…¥å®Œæˆï¼æˆåŠŸå¯¼å…¥ {imported_count} æ¡æ•°æ®ï¼Œè·³è¿‡ {skipped_count} æ¡ç©ºè¡Œï¼Œå¤±è´¥ {error_count} æ¡',
            'imported_count': imported_count,
            'error_count': error_count,
            'skipped_count': skipped_count
        }
        
        logger.info(f'ğŸ“Š å¯¼å…¥ç»Ÿè®¡: æˆåŠŸ {imported_count} æ¡ï¼Œè·³è¿‡ {skipped_count} æ¡ï¼Œå¤±è´¥ {error_count} æ¡')
        
        if error_messages:
            result['error_messages'] = error_messages[:10]
        
        return JsonResponse(result)
        
    except Exception as e:
        logger.error(f'å¯¼å…¥{module_name}å¤±è´¥: {str(e)}', exc_info=True)
        return JsonResponse({'status': 'error', 'message': f'å¯¼å…¥å¤±è´¥: {str(e)}'}, status=500)


# ==================== è¿œé€šæŠ¥è¡¨åˆ—åæ˜ å°„é…ç½® ====================

YUANTONG_COLUMN_MAPPING = {
    # åŸºæœ¬å­—æ®µæ˜ å°„
    'æ—¥æœŸ': 'date', 'Date': 'date', 'æ£€æµ‹æ—¥æœŸ': 'date',
    'æ—¶é—´': 'time', 'Time': 'time', 'æ£€æµ‹æ—¶é—´': 'time',
    'ç­æ¬¡': 'shift', 'Shift': 'shift', 'Squad': 'shift', 'ç­ç»„': 'shift', 'ç­åˆ«': 'shift',
    'äº§å“åç§°': 'product_name', 'Product Name': 'product_name', 'Grade': 'product_name', 
    'äº§å“å‹å·': 'product_name',
    'åŒ…è£…ç±»å‹': 'packaging', 'Packaging': 'packaging', 'IPKP CODE': 'packaging',
    'æ‰¹å·': 'batch_number', 'Batch Number': 'batch_number', 'æ‰¹æ¬¡å·': 'batch_number', 
    'LOT': 'batch_number', 'LOTæ‰¹å·': 'batch_number', 'æ‰¹å·/æ—¥æœŸ': 'batch_number', 'æ‰¹æ¬¡': 'batch_number',
    'ç‰©æ–™ç±»å‹': 'material_type', 'Material Type': 'material_type',
    
    # æ£€æµ‹æ•°æ®å­—æ®µæ˜ å°„
    'çƒ˜å¹²ååŸåœŸæ°´åˆ† (%)': 'moisture_after_drying',
    'çƒ˜å¹²ååŸåœŸæ°´åˆ†ï¼ˆ%ï¼‰': 'moisture_after_drying',
    'çƒ˜å¹²ååŸåœŸæ°´åˆ†(%)': 'moisture_after_drying',
    'å¹²ç‡¥ååŸåœŸæ°´åˆ†(%)': 'moisture_after_drying',
    'å¹²ç‡¥ååŸåœŸæ°´åˆ†ï¼ˆ%ï¼‰': 'moisture_after_drying',
    'çƒ˜å¹²ååŸ': 'moisture_after_drying',
    'Moisture after drying': 'moisture_after_drying', 
    'å¹²ç‡¥ååŸåœŸæ°´åˆ†': 'moisture_after_drying',
    
    # å…¥çª‘å‰ç¢±å«é‡
    'å…¥çª‘å‰ç¢±å«é‡(%)': 'alkali_content', 
    'å…¥çª‘å‰ç¢±å«é‡': 'alkali_content',
    'å…¥çª‘å‰': 'alkali_content',
    'å«é‡ (%)': 'alkali_content',
    'Alkali content (%)': 'alkali_content',
    
    'åŠ©å‰‚æ·»åŠ æ¯”ä¾‹': 'flux',
    'åŠ©æº¶å‰‚æ·»åŠ æ¯”ä¾‹': 'flux', 
    'åŠ©æº¶å‰‚': 'flux', 
    'åŠ©ç£¨å‰‚æ·»åŠ æ¯”ä¾‹': 'flux',
    '*flux agent': 'flux', 
    'flux agent addition ratio': 'flux',
    
    # è¿œé€šç‰¹æœ‰å­—æ®µ
    'è¿œé€šæ¸—é€ç‡ç³»æ•°': 'yuantong_permeability_coefficient',
    'è¿œé€šæ ·å“é‡é‡': 'yuantong_sample_weight',
    'è¿œé€šè¿‡æ»¤é¢ç§¯': 'yuantong_filter_area',
    
    # æ¸—é€ç‡
    'è¿é€šæ»¤é€Ÿç‡': 'permeability',
    'è¿ˆé€šæ¸—é€ç‡': 'permeability',
    'è¿œé€šæ¸—é€ç‡(Darcy)': 'permeability', 
    'è¿œé€šæ¸—é€ç‡': 'permeability',
    'Permeability_1': 'permeability',
    'é•¿é«˜æ»¤é€Ÿç‡': 'permeability_long',
    'é•¿å¯Œæ¸—é€ç‡(Darcy)': 'permeability_long', 
    'é•¿å¯Œæ¸—é€ç‡': 'permeability_long',
    'Permeability_2': 'permeability_long',
    
    # è¿‡æ»¤ç›¸å…³
    'è¿‡æ»¤æ—¶é—´(ç§’)': 'filter_time', 'è¿‡æ»¤æ—¶é—´': 'filter_time', 'Filter Time': 'filter_time',
    'æ°´é»åº¦(mPa.s)': 'water_viscosity', 'æ°´é»åº¦': 'water_viscosity', 'Water Viscosity': 'water_viscosity',
    'é¥¼åš(mm)': 'cake_thickness', 'é¥¼åš': 'cake_thickness', 'Cake Thickness': 'cake_thickness',
    
    # é¥¼å¯†åº¦
    'é¥¼å¯†åº¦(g/cm3)': 'wet_cake_density', 
    'é¥¼å¯†åº¦ï¼ˆg/cm3ï¼‰': 'wet_cake_density',
    'é¥¼å¯†åº¦': 'wet_cake_density', 
    'ç­›å¯†åº¦(g/cm3)': 'wet_cake_density',
    'Wet cake density': 'wet_cake_density',
    'è¿œé€šé¥¼å¯†åº¦(g/cm3)': 'yuantong_cake_density', 
    'è¿œé€šé¥¼å¯†åº¦ï¼ˆg/cm3ï¼‰': 'yuantong_cake_density',
    'è¿œé€šé¥¼å¯†åº¦ (g/cm3)': 'yuantong_cake_density',
    'è¿œé€šé¥¼å¯†åº¦(g/cmÂ³)': 'yuantong_cake_density',
    'è¿œé€šé¥¼å¯†åº¦ï¼ˆg/cmÂ³ï¼‰': 'yuantong_cake_density',
    'è¿œé€šé¥¼å¯†åº¦': 'yuantong_cake_density',
    'é•¿å¯Œé¥¼å¯†åº¦(g/cm3)': 'changfu_cake_density', 
    'é•¿å¯Œé¥¼å¯†åº¦ï¼ˆg/cm3ï¼‰': 'changfu_cake_density',
    'é•¿å¯Œé¥¼å¯†åº¦ (g/cm3)': 'changfu_cake_density',
    'é•¿å¯Œé¥¼å¯†åº¦(g/cmÂ³)': 'changfu_cake_density',
    'é•¿å¯Œé¥¼å¯†åº¦ï¼ˆg/cmÂ³ï¼‰': 'changfu_cake_density',
    'é•¿å¯Œé¥¼å¯†åº¦': 'changfu_cake_density',
    
    'æŒ¯å®å¯†åº¦(g/cm3)': 'bulk_density',
    'æŒ¯å®å¯†åº¦ï¼ˆg/cm3)': 'bulk_density',
    'æŒ¯å®å¯†åº¦ï¼ˆg/cm3ï¼‰': 'bulk_density',
    'æŒ¯å®å¯†åº¦': 'bulk_density',
    'é‡åº¦ (k) 14W': 'bulk_density',
    'ç°å€¼ (c/m)': 'bulk_density',
    
    # ç™½åº¦
    'ç™½åº¦': 'brightness', 
    'Bri.': 'brightness', 
    'Brightness': 'brightness',
    
    # å¯å¡‘åº¦å¯èƒ½æ˜¯æ¶¡å€¼
    'å¯å¡‘åº¦ (c/cm)': 'swirl',
    'å¯å¡‘åº¦': 'swirl',
    'æ¶¡å€¼(cm)': 'swirl',
    'æ¶¡å€¼ï¼ˆcmï¼‰': 'swirl',
    'æ»¤å€¼(cm)': 'swirl',
    'æ¶¡å€¼': 'swirl', 
    'Swirl (cm)': 'swirl', 
    'Swirl': 'swirl',
    
    'æ°”å‘³': 'odor', 
    'Odor': 'odor',
    
    # ç”µå¯¼å€¼å’ŒpH
    'ç”µå¯¼å€¼ (as/c pH': 'conductance',
    'ç”µå¯¼å€¼(ms/cm)': 'conductance', 
    'ç”µå¯¼å€¼(ms/c pll': 'conductance',
    'ç”µå¯¼å€¼': 'conductance', 
    'Conductance (ms/c)': 'conductance', 
    'Conductance': 'conductance',
    'pH': 'ph', 
    'pHå€¼': 'ph',
    
    'æ°´åˆ†(%)': 'moisture', 
    'æ°´åˆ†': 'moisture', 
    'Moisture (%)': 'moisture', 
    'Moisture': 'moisture',
    
    'æ‰¹æ•°': 'bags',
    'è¢‹æ•°': 'bags', 
    'çƒ§æ•°': 'bags',
    'Bags': 'bags',
    
    'å¨': 'tons', 
    'å¨æ•°': 'tons',
    'Tons': 'tons', 
    'äº§é‡': 'tons',
    
    # ç­›åˆ†æ•°æ®å­—æ®µæ˜ å°„
    '14W': 'sieving_14m',
    '+14M (%)': 'sieving_14m', 
    '+14M': 'sieving_14m', 
    '14M': 'sieving_14m',
    '+30M (%)': 'sieving_30m', 
    '+30M': 'sieving_30m', 
    '30M': 'sieving_30m',
    '+40M (%)': 'sieving_40m', 
    '+40M': 'sieving_40m', 
    '40M': 'sieving_40m',
    'M': 'sieving_40m',
    '+80M (%)': 'sieving_80m', 
    '+80M': 'sieving_80m', 
    '80M': 'sieving_80m',
    '+100M (%)': 'sieving_100m', 
    '+100M': 'sieving_100m', 
    '100M': 'sieving_100m',
    '+150M (%)': 'sieving_150m', 
    '+150M': 'sieving_150m',
    '150M': 'sieving_150m',
    '150M ': 'sieving_150m',
    '+200M (%)': 'sieving_200m', 
    '+200M': 'sieving_200m', 
    '200M': 'sieving_200m',
    '200M ': 'sieving_200m',
    '+325M (%)': 'sieving_325m', 
    '+325M': 'sieving_325m', 
    '325M': 'sieving_325m',
    '325M ': 'sieving_325m',
    
    # ç¦»å­æ•°æ®å­—æ®µæ˜ å°„
    'é“ç¦»å­ (mg/é’™ç¦»å­ (mg/é“ç¦»å­ (mg/ç™½åº¦': 'fe_ion',
    'é“ç¦»å­ï¼ˆmg/kgï¼‰': 'fe_ion',
    'é“ç¦»å­(mg/kg)': 'fe_ion',
    'é“ç¦»å­(mg/': 'fe_ion',
    'é“ç¦»å­': 'fe_ion', 
    'Feç¦»å­': 'fe_ion', 
    'Fe': 'fe_ion',
    'é’™ç¦»å­ï¼ˆmg/kgï¼‰': 'ca_ion',
    'é’™ç¦»å­(mg/kg)': 'ca_ion',
    'é’™ç¦»å­(mg/': 'ca_ion',
    'é’™ç¦»å­': 'ca_ion',
    'Caç¦»å­': 'ca_ion', 
    'Ca': 'ca_ion',
    'é“ç¦»å­ï¼ˆmg/kgï¼‰': 'al_ion',
    'é“ç¦»å­(mg/kg)': 'al_ion',
    'é“ç¦»å­(mg/': 'al_ion',
    'é“ç¦»å­': 'al_ion',
    'Alç¦»å­': 'al_ion', 
    'Al': 'al_ion',
    
    'å¸æ²¹ç‡ (%)': 'oil_absorption',
    'å¸æ²¹ç‡ï¼ˆ%ï¼‰': 'oil_absorption',
    'å¸æ²¹é‡': 'oil_absorption', 
    'å¸æ²¹ç‡(%)': 'oil_absorption',
    'å¸æ°´ç‡ (%)': 'water_absorption',
    'å¸æ°´ç‡ï¼ˆ%ï¼‰': 'water_absorption',
    'å¸æ°´é‡': 'water_absorption', 
    'å¸æ°´ç‡(%)': 'water_absorption',
    
    'å¤‡æ³¨': 'remarks', 
    'Remarks': 'remarks', 
    'Notes': 'remarks'
}

# è¿œé€šå­—æ®µæ˜¾ç¤ºåç§°æ˜ å°„
YUANTONG_FIELD_DISPLAY_NAMES = {
    'date': 'æ—¥æœŸ',
    'time': 'æ—¶é—´',
    'shift': 'ç­æ¬¡',
    'product_name': 'äº§å“åç§°',
    'packaging': 'åŒ…è£…ç±»å‹',
    'batch_number': 'æ‰¹å·',
    'material_type': 'ç‰©æ–™ç±»å‹',
    'moisture_after_drying': 'å¹²ç‡¥ååŸåœŸæ°´åˆ†(%)',
    'alkali_content': 'å…¥çª‘å‰ç¢±å«é‡(%)',
    'flux': 'åŠ©æº¶å‰‚æ·»åŠ æ¯”ä¾‹',
    'yuantong_permeability_coefficient': 'è¿œé€šæ¸—é€ç‡ç³»æ•°',
    'yuantong_sample_weight': 'è¿œé€šæ ·å“é‡é‡',
    'yuantong_filter_area': 'è¿œé€šè¿‡æ»¤é¢ç§¯',
    'permeability': 'è¿œé€šæ¸—é€ç‡(Darcy)',
    'permeability_long': 'é•¿å¯Œæ¸—é€ç‡(Darcy)',
    'filter_time': 'è¿‡æ»¤æ—¶é—´(ç§’)',
    'water_viscosity': 'æ°´é»åº¦(mPa.s)',
    'cake_thickness': 'é¥¼åš(mm)',
    'wet_cake_density': 'é¥¼å¯†åº¦(g/cm3)',
    'yuantong_cake_density': 'è¿œé€šé¥¼å¯†åº¦(g/cm3)',
    'changfu_cake_density': 'é•¿å¯Œé¥¼å¯†åº¦(g/cm3)',
    'bulk_density': 'æŒ¯å®å¯†åº¦(g/cm3)',
    'brightness': 'ç™½åº¦',
    'swirl': 'æ¶¡å€¼(cm)',
    'odor': 'æ°”å‘³',
    'conductance': 'ç”µå¯¼å€¼(ms/cm)',
    'ph': 'pH',
    'moisture': 'æ°´åˆ†(%)',
    'bags': 'è¢‹æ•°',
    'tons': 'å¨æ•°',
    'sieving_14m': '+14M',
    'sieving_30m': '+30M',
    'sieving_40m': '+40M',
    'sieving_80m': '+80M',
    'sieving_100m': '+100M',
    'sieving_150m': '+150M',
    'sieving_200m': '+200M',
    'sieving_325m': '+325M',
    'fe_ion': 'Feç¦»å­',
    'ca_ion': 'Caç¦»å­',
    'al_ion': 'Alç¦»å­',
    'oil_absorption': 'å¸æ²¹ç‡(%)',
    'water_absorption': 'å¸æ°´ç‡(%)',
    'remarks': 'å¤‡æ³¨',
}


def import_yuantong_report_data(request, model_class, module_name, log_module_code):
    """
    é€šç”¨çš„è¿œé€šæŠ¥è¡¨å¯¼å…¥å‡½æ•°
    
    Args:
        request: Djangoè¯·æ±‚å¯¹è±¡
        model_class: æ¨¡å‹ç±»ï¼ˆYuantongQCReportæˆ–Yuantong2QCReportï¼‰
        module_name: æ¨¡å—åç§°ï¼ˆç”¨äºé”™è¯¯æç¤ºï¼‰
        log_module_code: æ—¥å¿—æ¨¡å—ä»£ç ï¼ˆ'yuantong'æˆ–'yuantong2'ï¼‰
        
    Returns:
        JsonResponse: å¯¼å…¥ç»“æœ
    """
    if request.method != 'POST':
        return JsonResponse({'status': 'error', 'message': 'ä»…æ”¯æŒPOSTè¯·æ±‚'}, status=405)
    
    try:
        # æ£€æŸ¥æ˜¯å¦æœ‰ä¸Šä¼ çš„æ–‡ä»¶
        if 'excel_file' not in request.FILES:
            return JsonResponse({'status': 'error', 'message': 'è¯·é€‰æ‹©è¦å¯¼å…¥çš„Excelæ–‡ä»¶'}, status=400)
        
        excel_file = request.FILES['excel_file']
        
        # æ£€æŸ¥æ–‡ä»¶æ‰©å±•å
        if not excel_file.name.endswith(('.xlsx', '.xls')):
            return JsonResponse({'status': 'error', 'message': 'ä»…æ”¯æŒExcelæ–‡ä»¶æ ¼å¼(.xlsx, .xls)'}, status=400)
        
        # è¯»å–Excelæ–‡ä»¶
        df, use_pandas = read_excel_file(excel_file)
        
        # æ˜ å°„åˆ—å
        column_mapping = YUANTONG_COLUMN_MAPPING.copy()
        df_mapped = map_excel_columns(df, column_mapping, use_pandas)
        
        # å¤„ç†æ•°æ®å¹¶å¯¼å…¥
        imported_count = 0
        error_count = 0
        error_messages = []
        skipped_count = 0
        
        # å¯¼å…¥validate_field_by_modelå‡½æ•°
        from home.utils.validators import validate_field_by_model
        
        # å¤„ç†æ¯ä¸€è¡Œæ•°æ®
        for index, row_obj in df_mapped.iterrows():
            try:
                row = row_obj
                
                # åˆ›å»ºget_row_valueçš„é—­åŒ…å‡½æ•°
                def get_row_value_func(key):
                    return get_row_value(row, key, use_pandas, column_mapping)
                
                # æ£€æŸ¥æ˜¯å¦æ˜¯æç¤ºä¿¡æ¯è¡Œ
                date_val = get_row_value_func('date')
                if is_hint_row(date_val, use_pandas):
                    skipped_count += 1
                    logger.debug(f'è·³è¿‡ç¬¬ {index + 2} è¡Œï¼šæç¤ºä¿¡æ¯è¡Œ')
                    continue
                
                # æ£€æŸ¥å¿…å¡«å­—æ®µï¼šdateå’Œproduct_name
                product_name_val = get_row_value_func('product_name')
                if not (date_val and is_notna(date_val, use_pandas)):
                    error_count += 1
                    error_msg = f'ç¬¬ {index + 2} è¡Œç¼ºå°‘å¿…å¡«å­—æ®µ: æ—¥æœŸ'
                    error_messages.append(error_msg)
                    logger.error(f'å¯¼å…¥{module_name}å¤±è´¥: {error_msg}')
                    continue
                
                if not (product_name_val and is_notna(product_name_val, use_pandas)):
                    error_count += 1
                    error_msg = f'ç¬¬ {index + 2} è¡Œç¼ºå°‘å¿…å¡«å­—æ®µ: äº§å“åç§°'
                    error_messages.append(error_msg)
                    logger.error(f'å¯¼å…¥{module_name}å¤±è´¥: {error_msg}')
                    continue
                
                # å¤„ç†æ—¥æœŸå’Œæ—¶é—´
                date_obj = process_date_value(date_val, use_pandas)
                if not date_obj:
                    error_count += 1
                    error_msg = f'ç¬¬ {index + 2} è¡Œæ—¥æœŸæ ¼å¼é”™è¯¯'
                    error_messages.append(error_msg)
                    logger.error(f'å¯¼å…¥{module_name}å¤±è´¥: {error_msg}')
                    continue
                
                time_val = get_row_value_func('time')
                time_obj = process_time_value(time_val, use_pandas)
                
                # æ„å»ºæ•°æ®å­—å…¸
                data = {
                    'date': date_obj,
                    'time': time_obj,
                    'product_name': str(product_name_val).strip(),
                }
                
                # å¤„ç†å­—ç¬¦ä¸²å­—æ®µ
                string_fields = ['shift', 'packaging', 'batch_number', 'flux', 'remarks', 'material_type']
                for field in string_fields:
                    val = get_row_value_func(field)
                    if val and is_notna(val, use_pandas):
                        data[field] = str(val)
                    else:
                        if field == 'material_type':
                            data[field] = 'åŠ©ç†”ç……çƒ§å“'
                        else:
                            data[field] = ''
                
                # å¤„ç†å…¥çª‘å‰ç¢±å«é‡
                alkali_val = get_row_value_func('alkali_content')
                data['alkali_content'] = process_numeric_value(alkali_val, use_pandas)
                
                # å¤„ç†æ•°å­—å­—æ®µ
                numeric_fields = [
                    'moisture_after_drying', 'yuantong_permeability_coefficient', 'yuantong_sample_weight', 
                    'yuantong_filter_area', 'permeability', 'permeability_long', 'filter_time', 
                    'water_viscosity', 'cake_thickness', 'wet_cake_density', 'yuantong_cake_density', 
                    'changfu_cake_density', 'bulk_density', 'brightness', 'swirl', 'odor',
                    'conductance', 'ph', 'moisture', 'bags', 'tons', 'fe_ion', 'ca_ion',
                    'al_ion', 'oil_absorption', 'water_absorption', 'sieving_14m', 'sieving_30m',
                    'sieving_40m', 'sieving_80m'
                ]
                
                for field in numeric_fields:
                    val = get_row_value_func(field)
                    data[field] = process_numeric_value(val, use_pandas)
                
                # å¤„ç†ç­›åˆ†å­—æ®µï¼ˆå¯èƒ½æ˜¯å­—ç¬¦ä¸²ï¼‰
                sieving_fields = ['sieving_100m', 'sieving_150m', 'sieving_200m', 'sieving_325m']
                for field in sieving_fields:
                    val = get_row_value_func(field)
                    if val and is_notna(val, use_pandas):
                        data[field] = str(val)
                    else:
                        data[field] = ''
                
                # æ•°æ®æ ¡éªŒï¼šæ£€æŸ¥å¿…å¡«å­—æ®µ
                validation_errors = []
                field_display_names = YUANTONG_FIELD_DISPLAY_NAMES.copy()
                
                # æ£€æŸ¥æ‰€æœ‰å¿…å¡«å­—æ®µï¼ˆæ ¹æ®modelå®šä¹‰ï¼‰
                for field_name, field_value in data.items():
                    if field_name in ['user', 'username']:
                        continue
                    
                    field_display_name = field_display_names.get(field_name, field_name)
                    is_valid, error_msg = validate_field_by_model(
                        model_class, 
                        field_name, 
                        field_value, 
                        field_display_name
                    )
                    
                    if not is_valid:
                        validation_errors.append(f'{field_display_name}: {error_msg}')
                
                if validation_errors:
                    error_count += 1
                    error_msg = f'ç¬¬ {index + 2} è¡Œæ•°æ®æ ¡éªŒå¤±è´¥: {"; ".join(validation_errors)}'
                    error_messages.append(error_msg)
                    logger.error(f'å¯¼å…¥{module_name}æ•°æ®æ ¡éªŒå¤±è´¥: {error_msg}')
                    continue
                
                # è®¾ç½®ç”¨æˆ·ä¿¡æ¯
                data['user'] = request.user
                data['username'] = request.user.username
                
                # åˆ›å»ºè®°å½•
                model_class.objects.create(**data)
                imported_count += 1
                
            except Exception as e:
                error_count += 1
                error_msg = f'ç¬¬ {index + 2} è¡Œå¯¼å…¥å¤±è´¥: {str(e)}'
                error_messages.append(error_msg)
                logger.error(f'å¯¼å…¥{module_name}å¤±è´¥: {error_msg}', exc_info=True)
        
        # è®°å½•æ“ä½œæ—¥å¿—
        from home.models import UserOperationLog
        UserOperationLog.log_operation(
            request, 'CREATE', log_module_code, None,
            f'æ‰¹é‡å¯¼å…¥Excelæ•°æ®: æˆåŠŸ{imported_count}æ¡, å¤±è´¥{error_count}æ¡'
        )
        
        result = {
            'status': 'success',
            'message': f'å¯¼å…¥å®Œæˆï¼æˆåŠŸå¯¼å…¥ {imported_count} æ¡æ•°æ®ï¼Œè·³è¿‡ {skipped_count} æ¡ç©ºè¡Œï¼Œå¤±è´¥ {error_count} æ¡',
            'imported_count': imported_count,
            'error_count': error_count,
            'skipped_count': skipped_count
        }
        
        logger.info(f'ğŸ“Š å¯¼å…¥ç»Ÿè®¡: æˆåŠŸ {imported_count} æ¡ï¼Œè·³è¿‡ {skipped_count} æ¡ï¼Œå¤±è´¥ {error_count} æ¡')
        
        if error_messages:
            result['error_messages'] = error_messages[:10]
        
        return JsonResponse(result)
        
    except Exception as e:
        logger.error(f'å¯¼å…¥{module_name}å¤±è´¥: {str(e)}', exc_info=True)
        return JsonResponse({'status': 'error', 'message': f'å¯¼å…¥å¤±è´¥: {str(e)}'}, status=500)
